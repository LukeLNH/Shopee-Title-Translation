{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom tensorflow import keras\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/shopee-title-translation/\")\nshopee_data = pd.concat([pd.read_csv(\"dev_tcn.csv\").drop(columns = [\"split\"]), pd.read_csv(\"dev_en.csv\")], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shopee_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 107","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ch_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = \"<OOV>\")\n#ch_tokenizer.fit_on_texts(shopee_data.text)\nch_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(shopee_data.text, target_vocab_size=2e15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = \"<OOV>\")\n# en_tokenizer.fit_on_texts(shopee_data.translation_output)\nen_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(shopee_data.translation_output, target_vocab_size=2e15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shopee_data.text = ch_tokenizer.texts_to_sequences(shopee_data.text)\n# shopee_data.translation_output = en_tokenizer.texts_to_sequences(shopee_data.translation_output)\nshopee_data.text = shopee_data.text.apply(ch_tokenizer.encode)\nshopee_data.translation_output = shopee_data.translation_output.apply(en_tokenizer.encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding start and end tokens\nshopee_data.translation_output = shopee_data.translation_output.apply(lambda en : [en_tokenizer.vocab_size] + en + [en_tokenizer.vocab_size + 1])\nshopee_data.text = shopee_data.text.apply(lambda ch : [ch_tokenizer.vocab_size] + ch + [ch_tokenizer.vocab_size + 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shopee_data.text = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(shopee_data.text, maxlen = 30, padding = \"post\")))\nshopee_data.translation_output = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(shopee_data.translation_output, maxlen = 30, padding = \"post\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert to tf dataset to use with the code in the model (the model expects a tf dataset)\n#its possible to modify the model to expect pandas dataframe instead, but that's a lot more work than converting the data to tf dataset\n    #for now\n    \ntf_shopee_data = tf.data.Dataset.from_tensor_slices((list(shopee_data.text), list(shopee_data.translation_output)))\ntf_shopee_data = tf_shopee_data.cache() #so loading the data is quicker\ntf_shopee_data = tf_shopee_data.shuffle(100).padded_batch(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#positional encoding\n#code from https://www.tensorflow.org/tutorials/text/transformer#positional_encoding\n#added to give the model context of where each word is in the sentence, since the model doesnt have convolutions or lstm\n\n#Signature: 2DArray, 2DArray, Num\n#Effects: return an n x m matrix, where n is the number of pos elements, and m is the dimension of each pos element\ndef get_angles(pos, i, model_depth):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(model_depth))\n    return pos * angle_rates\n\n#Signature: Num, Num\n            #total num words, num of layers in model\n#Effects: Returns a modified result of get_angles to reflect positional encoding\ndef positional_encoding(position, model_depth):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          model_depth)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Masking\n#Code from https://www.tensorflow.org/tutorials/text/transformer#masking\n\n#Mask all padded tokens so that the model doesn't read the extra tokens as input\n\n#Signature: listOfNum (listOfTokens)\n#Effects: Produces 1 if the token in the sequence is a padding token, 0 otherwise\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\n#Signature: Num \n#Effects: Produces a size x size matrix where each sequential array unmasks the next word in the sequence\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_padding_mask(tf.convert_to_tensor(list(shopee_data.text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_look_ahead_mask(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dot Product Attention\n#Code from https://www.tensorflow.org/tutorials/text/transformer#scaled_dot_product_attention\n\n#Takes in 3 inputs: Query, Key, Value and Mask. Q, K, V are calculated by passing the input separatly through wq,wk,wv - Dense layers in\n    #multiheaded attention. Mask refers to the 2 functions defined above.\n    \n#K and Q are used to calculate the Attention Weights. The Attention Weights matrix-multiplied with V to then produce the output\n#Since softmax normalization is done on K, the value of K decides the \"importance\" given to Q\n#In short, the attention weights calculate what words are most important in the sentence - multiplied by V, this ensures the output keeps\n    #important words as is, and flushes out irrelevant words\n    \n#This particular dot product is scaled by the depth of the model because the deeper the model, the larger the variance of the matmul. To\n    #bring this variance back down, we scale the results down by the model depth. We want to scale it down because if the value of the matmul\n    #is very large, smaller gradients in the softmax essentially become irrelevant, and we get a very hard softmax. Scaling values down \n    #produces a much gentler softmax, which is what we want.\n    \n#The value of the masks is multiplied by a very large negative number so that the tokens we want to mask (represented by 1 in the sequence)\n    #becomes very negative, which then produces a value close to 0 after the softmax is applied (and thus attention will not be paid to \n    #these tokens, which is what we want :D)\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n    output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multihead Attention\n#https://www.tensorflow.org/tutorials/text/transformer#multi-head_attention\n\n#Each multihead attention receives inputs Q V K (All calculated from the original sequence using Dense Layers wq, wv, wk)\n    #The Multihead attention layer receives the input sequence 3 times to calculate q v k\n\n#Q V K is then split into heads, and each set of heads (set of Q V K) is passed through a dot product attention layer\n#The outputs of the attention layers (outputs of the heads) are then concatenated together, and passed through a final Dense layer\n#The output of the dense layer is passed through to the next multihead attention layer to be split into q v k again\n    \n#Multihead attention consists of 4 parts:\n    #Linear Layers containing wq wv wk, and afterwards split into heads\n    #scaled dot product attention layer\n    #concatenation of heads\n    #Final Dense Layer\n    \n#In Code, if we want to split into heads (e.g, split Q into heads), we would reshape Q into multiple sub-arrays and pass the entire\n    #masive array through the attention layer - python allows the dot product to attend to each sub-array individually. Afterwards, to\n    #concatenate the arrays back, we use transpose and reshape\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pointwise Feed Forward NN\n#a simple 2 layer NN, with Relu activation in between\n#part of the Encoder and Decoder Architecture\n#https://www.tensorflow.org/tutorials/text/transformer#point_wise_feed_forward_network\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoder Layer\n# https://www.tensorflow.org/tutorials/text/transformer#encoder_layer\n\n#Each encoder layer contains the following sublayers:\n    #Multihead Attention (with padding mask)\n    #residual connection from before Multihead Attention Layer + Normalization\n    \n    #Feedforward NN\n    #residual connection from before Multihead Attention Layer + Normalization\n    \n#Note (for decoder layers as well): Normalization is done after the residual connection is added\n#The Encoder has N of these encoder layers\n\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n        #dropout layers aren't necessary per se, but they're good for the NN (like residual connections). They randomly set outgoing\n            #edges of hidden nodes to 0 during training phase to reduce the chance of overfitting\n    def call(self, x, training, mask):\n\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decoder layer\n#https://www.tensorflow.org/tutorials/text/transformer#decoder_layer\n\n#Decoder layers have the following architecture:\n    #masked multiheaded attention (both padding and lookahead mask)\n    #Normalization + Residual Connection\n    \n    #multiheaded attention (padding mask): V and K are the ENCODER's output, while Q is the output from the first masked multihead\n        #attention layer after normalization. (Note: Encoder Output, not the attention weights)\n    #Normalization + Residual Connection\n    \n    #Feedforward network\n    #Normalization + Residual Connection\n    \n#The Decoder has N of these sublayers\n\n#Since the value of K determines the importance of Q (from dot product attention), The decoder is essentially looking at the Encoder's\n    #output (K) to determine the importance of it's own self-attended representation of the input (Q) to predict the next word.\n    \nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoder\n#https://www.tensorflow.org/tutorials/text/transformer#encoder\n\n#Architecture:\n    #Input Embedding\n    #Positional Encoding\n    # N x Encoder layers\n\n#The input is embedded and then summed with the positional encoding, and then passed onto the N Encoder layers\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.d_model)\n\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n\n        seq_len = tf.shape(x)[1]\n\n        # adding embedding and position encoding.\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decoder\n#https://www.tensorflow.org/tutorials/text/transformer#decoder\n\n#Architecture:\n    #Output Embedding (For cases where we're predicting, we only embed the start token. For training purposes, we may want to do\n        #teacher-forcing, where we embed the correct result for the next cycle, regardless of whether the model got it right or not)\n    #Positional Encoding (Again, for predicting, only positionally encode the start token)\n    #N x Decoder Layers\n    \n#The output of the final decoder layer is then passed through a final linear layer to predict what the next word is\n\n#The way the model (specifically the Decoder section) will train is as follows:\n    #The first token the decoder receives will be the start token\n    #afterwards, if the model is still training, we will concatenate the actual next word into the sentence and have the model predict\n        # the word. If the model isn't training, the model will concatenate its own prediction into the sentence and continue predicting\n        #the next word\n    #the above process is repeated until the model finishes translating the sentence\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                 look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the full transformer\n#https://www.tensorflow.org/tutorials/text/transformer#create_the_transformer\n#Architecture: Encode, Decoder, Final Linear Layer to output predictions\n\n# pe_input is positional encoding for the input, pe_output is for positional encoding of the output\n# training is a boolean, True if the model is training, false otherwise. Used for the dropout layer and to enforce teacher-forcing\n\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                               input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                               target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameters\nnum_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\n\ninput_vocab_size = ch_tokenizer.vocab_size + 2\ntarget_vocab_size = en_tokenizer.vocab_size + 2\ndropout_rate = 0.1\nEPOCHS = 120\nBATCH_SIZE = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Custome Optimizer\n#Normally I'd use a built in Adam optimizer - but I will follow the tutorial and use the optimizer with the learning rate scheduler\n#https://www.tensorflow.org/tutorials/text/transformer#optimizer\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loss and Metrics\n#https://www.tensorflow.org/tutorials/text/transformer#loss_and_metrics\n\n#We need to make sure to apply a pad masking when calculating the loss since the target sequences are padded\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n    name='train_accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training and checkpointing\n# https://www.tensorflow.org/tutorials/text/transformer#training_and_checkpointing\ntransformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)\n\n#This function isn't necessary per se, but it makes the code a lot cleaner. It essentially takes the masking functions defined at the start\n    #(create padding mask and create look ahead mask), and applies it to the input and output, and returns the 3 types of masks required by\n    #the Encoder and Decoder (encoding padding mask, decoding padding+look ahead mask, decoding padding mask)\ndef create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by \n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return enc_padding_mask, combined_mask, dec_padding_mask\n\n\n# Creating checkpoint paths\ncheckpoint_path = \"/kaggle/working/\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')\n\n\n\n# train_step_signature = [\n#     tf.TensorSpec(shape=(50, 30), dtype=tf.int32),\n#     tf.TensorSpec(shape=(50, 30), dtype=tf.int32),\n# ]\n\n# @tf.function(input_signature=train_step_signature)\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        print(\"predictions\")\n        predictions, _ = transformer(inp, tar_inp, \n                                     True, \n                                     enc_padding_mask, \n                                     combined_mask, \n                                     dec_padding_mask)\n        \n        print(\"loss\")\n        loss = loss_function(tar_real, predictions)\n\n    print(\"gradient descent\")\n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    print(\"metrics\")\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n    \n    \ndef train_transformer(tf_dataset, EPOCHS, verbose_interval):\n    for epoch in range(EPOCHS):\n        start = time.time()\n\n        train_loss.reset_states()\n        train_accuracy.reset_states()\n\n        # inp -> portuguese, tar -> english\n        for (batch, (inp, tar)) in enumerate(tf_dataset):\n            train_step(inp, tar)\n\n            if batch % verbose_interval == 0:\n                  print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n                  epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n\n        if (epoch + 1) % 5 == 0:\n            ckpt_save_path = ckpt_manager.save()\n            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                                 ckpt_save_path))\n\n        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                    train_loss.result(), \n                                                    train_accuracy.result()))\n\n        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n\ntrain_transformer(tf_shopee_data, EPOCHS = EPOCHS, verbose_interval = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.chdir(\"/kaggle/input/chinese-english-parallel-corpus-education-thesis/\")\n# thesis = pd.read_fwf(\"Bi-Thesis.txt\", header = None).iloc[:,0]\n# en_thesis = thesis.iloc[[i for i in range(0,50000,2)]]\n# ch_thesis = thesis.iloc[[i for i in range(1,50000,2)]]\n\n# # en_tokenizer.build_from_corpus(en_thesis, target_vocab_size=2e15)\n# # ch_tokenizer.build_from_corpus(ch_thesis, target_vocab_size=2e15)\n\n# en_thesis = en_thesis.apply(en_tokenizer.encode)\n# ch_thesis = ch_thesis.apply(ch_tokenizer.encode)\n\n# en_thesis = en_thesis.apply(lambda en : [en_tokenizer.vocab_size] + en + [en_tokenizer.vocab_size + 1])\n# en_thesis = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(en_thesis, padding = \"post\")))\n# ch_thesis = ch_thesis.apply(lambda ch : [ch_tokenizer.vocab_size] + ch + [ch_tokenizer.vocab_size + 1])\n# ch_thesis = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(ch_thesis, padding = \"post\")))\n\n# tf_thesis = tf.data.Dataset.from_tensor_slices((list(ch_thesis), list(en_thesis)))\n# tf_thesis = tf_thesis.cache()\n# tf_thesis = tf_thesis.shuffle(100).padded_batch(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Signature: pd.Series, pd.Series => tf.data.Dataset\ndef preprocessor(ch_data, en_data):\n    ch_data = ch_data.apply(ch_tokenizer.encode)\n    ch_data = ch_data.apply(lambda ch : [ch_tokenizer.vocab_size] + ch + [ch_tokenizer.vocab_size + 1])\n    ch_data = pd.Series(list(tf.keras.preprocessing.sequence.pad_sequences(ch_data, padding = \"post\"))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_transformer(tf_thesis, EPOCHS = 10, verbose_interval = 5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making predictions / evaluating\n#https://www.tensorflow.org/tutorials/text/transformer#evaluate\n\ndef evaluate(inp_sentence):\n    start_token = [ch_tokenizer.vocab_size]\n    end_token = [ch_tokenizer.vocab_size + 1]\n\n    #Adding start and end tokens to input sequence (in chinese)\n    inp_sentence = start_token + ch_tokenizer.encode(inp_sentence) + end_token\n    encoder_input = tf.expand_dims(inp_sentence, 0)\n\n    #Add English start token to the first word of the decoder\n    decoder_input = [en_tokenizer.vocab_size]\n    output = tf.expand_dims(decoder_input, 0)\n    \n    \n    #Code for the model to auto-regressively predict the sentence\n    for i in range(SEQ_LEN): #Defined at the start of the notebook, the longest sequence in ch that we have to predict\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n            encoder_input, output) #This is in the for loop because we will concatenate the prediction to the output until the end token\n                                    #is reached. Since the output changes per iteration, we create new masks for it entirely.\n\n            \n        # predictions.shape == (batch_size, seq_len, vocab_size)\n        predictions, attention_weights = transformer(encoder_input, \n                                                     output,\n                                                     False,\n                                                     enc_padding_mask,\n                                                     combined_mask,\n                                                     dec_padding_mask)\n\n        # select the last word from the seq_len dimension bc we're only interested in what it think the next word will be\n        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n        #argmax finds the index position of the output the model is most sure is correct\n            #axis = -1 means the last axis (the vocab_size axis)\n            #the transformer is effectively looking at the entire vocab and finding the index of the output it thinks is correct. This index\n                #is also the token ID\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        # If the Token ID is the end ID, return the sequence.\n        if predicted_id == en_tokenizer.vocab_size+1:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        #Otherwise, add the token it thinks is right to the end of the output. The output is then fed back into the Decoder\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    #If somehow we haven't reached the end token by the time the max sequence runs out, just return the output (sequence of tokens) that\n        #we've predicted so far\n    return tf.squeeze(output, axis=0), attention_weights\n\ndef translate(sentence):\n    result, attention_weights = evaluate(sentence) #Getting the preducted output tokens\n  \n    predicted_sentence = en_tokenizer.decode([i for i in result \n                                            if i < en_tokenizer.vocab_size])  #Decoding the output tokens into an english sentence\n\n    print('Input: {}'.format(sentence))\n    print('Predicted translation: {}'.format(predicted_sentence))\n    return predicted_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tcn = pd.read_csv(\"test_tcn.csv\").text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tcn = test_tcn.apply(ch_tokenizer.encode)\ntest_tcn = test_tcn.apply(lambda ch : [ch_tokenizer.vocab_size] + ch + [ch_tokenizer.vocab_size + 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"translate(test_tcn[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.Series(data= np.zeros(10000),name = \"product_title\")\nfor i in range(test_tcn.shape[0]):\n    print(i)\n    submission[i] = translate(test_tcn[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[j for j in range(5000,10000)]] = submission[[i for i in range(5000)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/working/submission.csv\")\nsubmission.name = \"product_title\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.name = \"product_title\"\ncategory = pd.read_csv(\"test_tcn.csv\").split\ncategory.name = \"category\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([submission, category], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.columns = [\"product_title\", \"category\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/working/submission2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = submission[pd.DataFrame.duplicated(submission)].index\nfor i in arr:\n    submission.loc[i] = submission.loc[i] + str(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}